# coding: utf-8

import pickle
import numpy as np
from datetime import datetime
#from keras.callbacks import TensorBoard
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.layers.embeddings import Embedding
from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard
from sklearn.model_selection import StratifiedKFold
from data_helper import text2sequence

def train(train_path, tokenizer_path):
    print('import data...')
    maxlen = 1024
    X, label, Y = text2sequence(train_path, tokenizer_path, maxlen)
    num_class = len(set(label))
    print('data import finished!')          
    tokenizer = pickle.load(open(tokenizer_path, 'rb'))
    num_words = len(tokenizer.word_index)+1
    print('prepare training data and validation data using k_fold')
    seed = 0
    k = 10
    k_fold = StratifiedKFold(n_splits = k, shuffle = True, random_state = seed)
    #10折交叉验证数据集划分
    
    cw_1 = {0:1, 1:1, 2:1, 3:1, 4:1, 5:1, 6:1, 7:1} #不考虑数据不均衡
    cw_2 = {0:0.348709, 1:3.457910, 2:1.451396, 3:2.116922,
    		4:17.358700, 5:0.404727, 6:3.370635, 7:1.167362} #每类权重为(1/8/该类出现频率)
    class_weight = [cw_1, cw_2] #使两种权重一样重要
    #在100个文档的数据集上测试发现不使用class_weight的效果比使用class_weight的好
    #使用class_weight的效果比只使用cw_2的效果好

    print('create lstm model...')
    model = Sequential()
    model.add(Embedding(num_words, 128, input_length=maxlen))
    model.add(Dropout(0.5))
    model.add(LSTM(64, recurrent_dropout=0.5))
    model.add(Dropout(0.5))
    model.add(Dense(num_class, activation='softmax'))
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    print(model.summary())    
    
    k_fold_cv_loss = []
    k_fold_cv_acc = []
        
    dt = datetime.now() 
    d = dt.date()
    h = dt.time().hour
    m = dt.time().minute
    time_str = '{}_{}{}'.format(d, h, m)
    
    mckpt = ModelCheckpoint('model/best-lstm_weights_{}.h5'.format(time_str), monitor = 'val_loss', mode = 'auto', verbose = 1,
    						save_best_only = True, save_weights_only = True, period = 1)
    rlstp = EarlyStopping(monitor = 'val_loss', patience = 3)
    tb= TensorBoard(log_dir='./logs', embeddings_freq=1, write_images = 1,
    							histogram_freq = 1, batch_size = 32)
    turn = 1
    for train, valid in k_fold.split(X,label):
        print('the {} turn training...'.format(turn))
        turn += 1
        model.fit(X[train], Y[train], validation_data = (X[valid], Y[valid]), class_weight = None,
                  callbacks = [mckpt], verbose = 2, epochs=11, batch_size=32)
        # Evaluate model
        loss, acc = model.evaluate(X[valid], Y[valid], verbose=0, batch_size=32)
        k_fold_cv_loss.append(loss)
        k_fold_cv_acc.append(acc)
        
    print("Model loss: {:0.6f}".format(np.mean(k_fold_cv_loss)))
    print("Model Accuracy: {:0.6f}%".format(np.mean(k_fold_cv_acc) * 100))

    # Save model
    model.save_weights('model/lstm_weights_{}.h5'.format(time_str))
    model.save('model/lstm_model_{}.h5'.format(time_str))
    with open('model/lstm_model_{}.json'.format(time_str), 'w') as outfile:
        outfile.write(model.to_json())

if __name__ == '__main__':

    train_path = 'data/train_set1.csv'
    tokenizer_path = 'build/tokenizer.pkl'
    train(train_path, tokenizer_path)

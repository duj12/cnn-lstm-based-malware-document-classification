# -*- coding: utf-8 -*-
"""
Created on Fri Dec 14 18:54:59 2018

@author: dujingb
"""

import os
import pickle
import json
import numpy as np
import pandas as pd
from keras.preprocessing.text import Tokenizer
from keras.preprocessing import sequence
from keras.utils import np_utils
from sklearn.preprocessing import LabelEncoder

# Preprocess dataset
def data_prep(in_path, out_path, isDropDuplicated = True, isTrainingData = True):
    '''
    将每个文件原始的api按照全局顺序(index列)串起来形成api序列
    可处理训练集和测试集文档
    in_path:输入原始文件路径
    out_path:输出api序列文件路径
    isDropDuplicated:是否去除同一文件在同一时间不同线程(tid)上调用的同一api，默认去除
    isTrainingData:是否是训练集，默认是
    ！！！注意，不去重时此函数运行在数据集上会导致4G内存不够用，因此重新编写不去重的函数
    '''
    if isTrainingData: #处理训练集数据
        fin = open(in_path)
        reader = pd.read_csv(fin, usecols = [0,1,2,4], iterator = True)#去掉tid列减小内存负担
        chunkSize = 100000
        chunks = []
        loop = True
        while loop:
            try :
                chunk = reader.get_chunk(chunkSize)
                chunks.append(chunk)
            except StopIteration:
                loop = False
                print('Iteration is stopped.')
        del chunk
        fin.close()
        dataframe = pd.concat(chunks, ignore_index = True) #拼接chunks
        del chunks
        #处理全局顺序，以file_id为主序，index为次序，对dataframe进行排序，就能得到API调用的全局顺序
        #但index相同的API没有先后顺序，可以相互调换，这里就使用原始文件中的顺序        
        dataframe.sort_values(['file_id','index'], axis = 0, inplace = True, ascending = True)
        #print(dataframe)
        #去掉完全重复的行
        if isDropDuplicated:
            dataframe = dataframe.drop_duplicates(keep = 'first') #去重
        dataframe.drop(columns = ['index'], inplace = True) #去掉index列
        #（全局顺序已经排好序了，不再需要index这一列了）
        #将同一个file_id的所有API串接起来形成新的行
        dataframe = dataframe.groupby(['file_id','label']).aggregate(lambda x: ' '.join(list(x)))
        #写入处理后的csv文件
        dataframe.to_csv(out_path)
        print('save csv_file finished!')      
        
    else:#处理测试集数据
        fin = open(in_path)
        reader = pd.read_csv(fin, usecols = [0,1,3], iterator = True)
        chunkSize = 100000
        chunks = []
        loop = True
        while loop:
            try :
                chunk = reader.get_chunk(chunkSize)
                chunks.append(chunk)
            except StopIteration:
                loop = False
                print('Iteration is stopped.')
        del chunk
        fin.close()
        dataframe = pd.concat(chunks, ignore_index = True) #拼接chunks
        del chunks
        #处理全局顺序，以file_id为主序，index为次序，对dataframe进行排序，就能得到API调用的全局顺序
        #但index相同的API没有先后顺序，可以相互调换，这里就使用原始文件中的顺序        
        dataframe.sort_values(['file_id','index'], axis = 0, inplace = True, ascending = True)
        #print(dataframe)
        #去掉完全重复的行
        if isDropDuplicated:
            dataframe = dataframe.drop_duplicates(keep = 'first')
        dataframe.drop(columns = ['index'], inplace = True)
        dataframe = dataframe.groupby(['file_id']).aggregate(lambda x: ' '.join(list(x)))
        dataframe.to_csv(out_path)
        print('save csv_file finished!')
        
def generate_api_sequence(in_path, out_path, isTrainingData = True):
	'''
	将原始输入文件的api按照全局顺序拼接起来
	'''
	if isTrainingData: #处理训练集数据
		fin = open(in_path)
		reader = pd.read_csv(fin, usecols = [0,1,2,4], iterator = True)#去掉tid列减小内存负担
		maxID = 100 #13887
		ID = 1
		for df in reader:#对可循环对象reader里的dataframe进行处理
			while ID <= maxID:
				dfi = df[df['file_id'] == ID] #选出每个ID的dataframe
            	#处理全局顺序，以file_id为主序，index为次序，对dataframe进行排序，就能得到API调用的全局顺序
            	#但index相同的API没有先后顺序，可以相互调换，这里就使用原始文件中的顺序  
				dfi.sort_values(['index'], axis = 0, inplace = True, ascending = True)
				dfi.drop(columns = ['index'], inplace = True) #去掉index列
            	#（全局顺序已经排好序了，不再需要index这一列了）
            	#将同一个file_id的所有API串接起来形成新的行
				dfi = dfi.groupby(['file_id','label']).aggregate(lambda x: ' '.join(list(x)))
				if ID == 1:
					dfi.to_csv(out_path, mode = 'a+')
				else:
					dfi.to_csv(out_path, header = False, mode = 'a+')
				ID += 1
    
		del dfi
		fin.close()
		print('save csv_file finished!')
    
	else:
		fin = open(in_path)
		reader = pd.read_csv(fin, usecols = [0,1,3], iterator = True)#去掉tid列减小内存负担
		maxID = 100 #12955
		ID = 1
		for df in reader:#对可循环对象reader里的dataframe进行处理
			while ID <= maxID:
				dfi = df[df['file_id'] == ID] #选出每个ID的dataframe
            	#处理全局顺序，以file_id为主序，index为次序，对dataframe进行排序，就能得到API调用的全局顺序
            	#但index相同的API没有先后顺序，可以相互调换，这里就使用原始文件中的顺序  
				dfi.sort_values(['index'], axis = 0, inplace = True, ascending = True)
				dfi.drop(columns = ['index'], inplace = True) #去掉index列
            	#（全局顺序已经排好序了，不再需要index这一列了）
            	#将同一个file_id的所有API串接起来形成新的行
				dfi = dfi.groupby(['file_id']).aggregate(lambda x: ' '.join(list(x)))
				if ID == 1:
					dfi.to_csv(out_path, mode = 'a+')
				else :
					dfi.to_csv(out_path, header = False, mode = 'a+')
				ID += 1
    
		del dfi
		fin.close()
		print('save csv_file finished!')

def generate_tokenizer(train_path, test_path, tokenizer_path, dict_path):
    '''
    生成tokenizer, 并保存所有text中出现的词到词典文件路径中
    '''
    train_text = list(pd.read_csv(train_path)['api'].values)
    test_text = list(pd.read_csv(test_path)['api'].values)
    
    tokenizer = Tokenizer()
    tokenizer.fit_on_texts(train_text + test_text)
    
    print("Save tokenizer...")
    if not os.path.exists(os.path.dirname(tokenizer_path)):
        os.makedirs(os.path.dirname(tokenizer_path))
    pickle.dump(tokenizer, open(tokenizer_path, "wb"))
    
    # Extract and save word dictionary
    if not os.path.exists(os.path.dirname(dict_path)):
        os.makedirs(os.path.dirname(dict_path))
    with open(dict_path, 'w') as outfile:
        json.dump(tokenizer.word_index, outfile, ensure_ascii=False)
    print('Save word dictionary finished!')

def over_sampling(in_path, out_path):
    '''
    对每种类别的文件分层采样，对样本数少那些类的进行整遍复制
    保证每种类别的文件数和第0类数目相近
    可统计每种标签的文件比例并分别保存每种文件
    输入原始未重采样的文档
    输出重采样后的文档
    '''
    dataframe = pd.read_csv(in_path)
    length = len(dataframe)
    length_0 = len(dataframe[dataframe['label']==0])
    print('import {} data'.format(length))
    print('the ratio of each type is:')
    for i in range(8):
        df_i = dataframe[dataframe['label']==i]
        length_i = len(df_i)
        #分开保存每种标签的文件
        #df_i.to_csv('data/label_{}.csv'.format(i))
        print('label: {}, ratio: {:.2f}'.format(i, length_i/length*100))
        append_times = int(length_0/(length_i+1))
        for j in range(append_times):
            dataframe = dataframe.append(df_i)
    dataframe.to_csv(out_path)
    print('after oversampling, we get {} data'.format(len(dataframe)))
    for i in range(8):
        df_i = dataframe[dataframe['label']==i]
        length_i = len(df_i)
        #分开保存每种标签的文件
        #df_i.to_csv('data/over_sampled_label_{}.csv'.format(i))
        print('label: {}, ratio: {:.2f}'.format(i, length_i/length*100))
    

def text2sequence(in_path, tokenizer_path, max_length):
    '''
    对训练数据集进行处理，将文本转成数值向量X并返回
    同时返回训练集标签向量y以及多分类one-hot向量Y
    '''
    tokenizer = pickle.load(open(tokenizer_path, 'rb'))
    encoder = LabelEncoder()
    text = pd.read_csv(in_path)['api'].values
    label = pd.read_csv(in_path)['label'].values
    print('transfer texts to sequences...then pad each sequence...')
    X = tokenizer.texts_to_sequences(text)
    X = sequence.pad_sequences(X, maxlen=max_length)
    Y = encoder.fit_transform(label)
    Y = np_utils.to_categorical(Y)
    return X, label, Y

def k_fold_validation_split(in_path, k):
    '''
    对输入文档进行k折数据集分割
    返回值为dataframe型的generator
    可通过for train_df, valid_df in k_fold_validation_split(in_path, k):
            do sth
    得到k组训练数据和验证数据集
    '''
    dataframe = pd.read_csv(in_path)
    length = len(dataframe)
    print('import {} data'.format(length))
    for j in range(k):
        train_set = []
        valid_set = []
        for i in range(8):
            df_i = dataframe[dataframe['label']==i]
            length_i = len(df_i)
            print('label: {}, ratio: {:.2f}'.format(i, length_i/length*100))
            valid_len = int(length_i/k)
            dfi_valid = df_i[j*valid_len:(j+1)*valid_len]
            dfi_train = pd.concat([df_i[:j*valid_len],df_i[(j+1)*valid_len:]], ignore_index = True)
            if not dfi_valid.empty:
                valid_set.append(dfi_valid)
            if not dfi_train.empty:
                train_set.append(dfi_train)
        train_df = pd.concat(train_set, ignore_index = True)
        valid_df = pd.concat(valid_set, ignore_index = True)
        yield train_df, valid_df


if __name__ == '__main__':

    in_path1 = 'mini_data/train.csv'
    out_path1 = 'mini_data/train_set1.csv'
    isTrainingData = True
    #isDropDuplicated = False
    #data_prep(in_path1, out_path1, isDropDuplicated, isTrainingData)
    generate_api_sequence(in_path1, out_path1, isTrainingData)
    
    in_path2 = out_path1
    out_path2 = 'data/over_sampled_train_set.csv'
    #over_sampling(in_path2, out_path2)
    
    in_path3= out_path2
    k = 10  
    #for train_df, valid_df in  k_fold_validation_split(in_path3, k):
        #print(len(valid_df))
        #print(len(train_df))
    
    train_path = 'data/train_set.csv'
    test_path = 'data/test_set.csv'
    tokenizer_path = 'build/tokenizer.pkl'
    dict_path = 'build/word-dictionary.json'
    #generate_tokenizer(train_path, test_path, tokenizer_path, dict_path)
    max_length = 1024
    
    
        
    